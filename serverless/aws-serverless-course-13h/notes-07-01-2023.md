# Kinesis stream

You can use Kinesis streaming services to ingest and process large volumes of data in near-real time. Clickstream data from mobile apps, application logs, and home security camera feeds are common examples where traditional batch processing would not give you the responsiveness you need to make data actionable.

The stream also provides an asynchronous data buffer for downstream processing.

There are three distinct Kinesis services for data processing: Kinesis Data Streams, Kinesis Data Firehose, and Kinesis Data Analytics, each with characteristics that lend themselves to different workloads.

All are fully managed and serverless. 

Kinesis Data Firehose and Kinesis Data Analytics were created to simplify common Kinesis data stream use cases. At a high level, producers add data records onto the stream and consumers get records and process them. You can create producers using the Kinesis Producer Library (KPL), the AWS SDK, and other third-party development tools.

On the consumer side, Lambda can be a consumer, streams can be consumers of other streams, and you can create your own consumer applications using the Kinesis Client Library (KCL).

Once a record is added to the stream, the record is available for a specified retention period that you can set per stream. The Kinesis Data Streams service scales horizontally by adding shards. Each shard has a uniquely identified sequence of data records, and each data record has a sequence number assigned by Kinesis.

Data records also include a partition key and a data blob. The partition key is used to group data by shard within a stream. The sequence number is unique per partition key within its shard. For writes, a shard can support up to 1,000 records per second, or up to a maximum of 1 MB per second. For reads, a shard can support up to five transactions per second, or up to a maximum of 2 MB per second.

The data capacity of your stream is determined by the number of shards configured on the stream. The total capacity of the stream is the sum of the capacity of its shards.

To increase throughput, you can either add shards or use aggregation to increase the number of records sent per API call, effectively increasing producer throughput. Aggregation refers to the storage of multiple records in a Kinesis Data Streams record. Consumers must de-aggregate the data in the record to act on it.

The KPL and KCL handle aggregation and de-aggregation of data seamlessly. And if you’re using Lambda functions as producers or consumers, you can use the Kinesis aggregation library available to Lambda rather than the KPL or KCL. This lets you de-aggregate records in your functions, so you can pull multiple records within a call and then de-aggregate them.

With Kinesis Data Firehose, you don't need to write consumer applications or manage shards. You configure your data producers to send data to Kinesis Data Firehose, and it automatically delivers the data to a destination that you specify.

You can also configure Kinesis Data Firehose to transform your data before delivering it.

Let’s look at a common event-driven pattern that uses Kinesis Data Firehose to collect clickstream data from a mobile ecommerce site, and create an asynchronous connection for processing by a Lambda function.

The client uses an API Gateway method that is integrated with a Kinesis Data Firehose stream.
API Gateway proxies the incoming records and loads data on to the Kinesis Data Firehose stream. Alternatively, the client could use the KPL and write data directly to the stream. Using API Gateway makes it easy to connect web or mobile clients and lets you decouple the API calls from the backend. should you decide to change it.
Kinesis Data Firehose delivers the raw data records directly to Amazon S3 via built-in integration. 
Amazon S3 triggers a Lambda function that transforms the data and then stores the transformed data on S3 and writes select data to DynamoDB.
The transformed data in Amazon S3 is then available for further analysis via tools like Amazon Athena. Alternatively, you might persist the transformed data to a data warehouse like Amazon Redshift for analytics.

This approach gives you the ability to capture large streams of real-time data, while handling further processing asynchronously. The details and variations of how you implement this pattern really depend on the nature of the data you are streaming, how the data is processed, and where the data should be stored.

Kinesis Data Firehose is a good choice here because it is designed for ingesting large volumes of data targeted for data stores. You can write directly to Amazon S3 as well as Amazon Redshift, Amazon Elasticsearch Service, and Splunk.

With Kinesis Data Firehose, you specify the amount of data or time period for which you want to buffer data. And Kinesis creates the number of shards you need, versus in Kinesis Data Streams where you specify how many shards to use.

Alternatively, you can configure Kinesis Data Firehose to perform data transformation via a specified Lambda function as the records are processed to streamline your architecture. Kinesis Data Firehose can also perform record format conversion on the fly before writing to a data store.

You can elect to convert JSON-formatted input data to Apache Parquet or Apache ORC output before storing streaming data to Amazon S3. This might work well if you just need to convert and store log data.

You could update the processing stream to include a Kinesis Data Analytics application before storing the data to Amazon S3. Kinesis Data Analytics lets you do real-time analysis using SQL before persisting the data. The service is designed for near real-time queries and lets you aggregate data across a sliding window.

With Kinesis Data Analytics, you write SQL statements or Apache Flink applications, then upload them into a Kinesis Data Analytics application to perform analysis on the data in the stream. Kinesis Data Analytics also supports using a Lambda function to preprocess the data before your SQL executes. 

For example, you might transform the data, enrich the data, or filter it. Input sources for a Kinesis Data Analytics application can either be a Kinesis Data Firehose stream or a Kinesis data stream, and Kinesis Data Analytics can output to those same types of streams. You can use Kinesis Data Analytics as an event source for Lambda.

This course doesn’t go deeper into creating streams or writing consumers, but there are resource links beneath the video. The key thing to remember is that your data processing never happens in isolation. The correct architecture depends on how the data will be used, transformed, and stored.

Kinesis Data Streams | Kinesis Data Firehose
-|-
You write custom consumers, with more targets |	The use case of transforming and storing streaming data is simplified
Order delivery and exactly-once delivery are guaranteed |	Order not guaranteed; messages could be delivered more than once
Failing messages block the shard until it succeeds or expires | Retry mechanisms are available for each delivery target
You set the number of shards | You set the data volume and the service manages the number of shards
Multiple consumers and multiple types of consumers are used | Stream is associated with a single destination

When you associate a Lambda function to a Data Firehose stream to transform data on the stream, Data Firehose tries the invocation three times and then skips that batch of records. Records that failed to process are delivered to an Amazon S3 bucket in a processing_failed folder.

- You control the number of shards with Kinesis Data Streams, whereas with Kinesis Data Firehose the number of shards is managed by the service based on data volume.
- A Kinesis Data Firehose stream is associated with a single destination, specified when you set up the stream, although you can configure an Amazon S3 bucket to receive a backup of source records. 

A Kinesis data stream can have multiple consumers and multiple types of consumers.
By default, all the consumers on a shard share the read throughput of that shard, and there is a limit of five consumers per shard.
If you add an enhanced fan-out consumer, it gets dedicated throughput, enabling you to support more than five consumers. But there is an additional cost for each enhanced fan-out consumer.

The Serverless Application Repository lets you quickly deploy reusable building blocks like the Event Fork Pipelines, and add your own components or applications to the repository to share within your account or publicly. You can use the repository to deploy your serverless applications as a set of nested applications.

Tasks like data backup and indexing data for search or analytics repeat across larger applications. Build those recurring patterns as smaller serverless applications and reuse them. Based on feedback from AWS customers about recurring patterns they needed for their serverless applications, we built a set of applications called Event Fork Pipelines and made them available in the Serverless Application Repository.

https://www.youtube.com/watch?v=TXh5oU_yo9M&feature=youtu.be

https://www.youtube.com/watch?v=TXh5oU_yo9M&feature=youtu.be

https://docs.aws.amazon.com/eventbridge/latest/userguide/content-filtering-with-event-patterns.html

For a summary of important pieces of messaging and streaming, choose the appropriate tab.
messaging
- The core entity is an individual message, and message rates vary.
- Messages are deleted when they’ve been consumed.
- Configure retries and dead-letter queues for failures.

streaming
- You look at the stream of messages together, and the stream is generally continuous.
- Data remains on the stream for a period of time. Consumers must maintain a pointer.
- A message is retried until it succeeds or expires. You must build error handling into your function to bypass a record.

https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html

https://docs.aws.amazon.com/streams/latest/dev/introduction.html

https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html

https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html

https://docs.aws.amazon.com/en_pv/kinesisanalytics/latest/dev/lambda-preprocessing.html
