# I-PAM

how are the pods and the bridge networks assigned IP

2 methods
- DHCP
- host-local

```bash
$ ip addr show weave 

4: weave: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue state UP group default qlen 1000
    link/ether 26:16:66:43:f0:03 brd ff:ff:ff:ff:ff:ff
    inet 10.244.0.1/16 brd 10.244.255.255 scope global weave
       valid_lft forever preferred_lft forever


$ ip route | grep the CNI deployed weave cni gateway

default via 172.25.0.1 dev eth1 
10.244.0.0/16 dev weave proto kernel scope link src 10.244.192.0 
172.25.0.0/24 dev eth1 proto kernel scope link src 172.25.0.93 
192.25.137.0/24 dev eth0 proto kernel scope link src 192.25.137.12
```

# k8s services

Kubeproxy

here the already decided range of IP present and added to the service where the IP of the service is added to the routing table for forwarding traffic when anything hits this IP to the IP of Pod

also, services are cluster-wide IP is assigned

> its not just IP its a IP and port combination

 How are these rules created

multiple ways Kubeproxy does it 

- userspace - Kubeproxy listens on a port for each service and proxies connections to the pods. By creating ipvs rules
- thrid and tehe default option and the one fimilar to use is using IP tables. the proxy mode can be set using the proxy mode option while confiuring the kube-proxy service if this is not set, it defaults to iptables

```bash
kube-proxy --proxy-mode [userspace | iptables | ipvs]...
```

the IP ranges for the services and the pods should never overlap
for example

```bash
ps aux | grep kube-api-server

# --service-cluster-ip-range=10.96.0.0/12
# 10.96.0.0 => 10.111.255.255
# if not specified the default is 10.0.0.0/24
```

# FAQ
one way to do this is to make use of the ipcalc utility. If it is not installed, you can install it by running:

## get the IP ranges of nodes
```bash
apt update && apt install ipcalc
```
Then use it to determine the network range

```bash

controlplane ~ ➜  apt install ipcalc
Reading package lists... Done
Building dependency tree       
Reading state information... Done
ipcalc is already the newest version (0.41-5).
0 upgraded, 0 newly installed, 0 to remove and 75 not upgraded.

controlplane ~ ➜  ip a | grep eth0
9265: eth0@if9266: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default 
    inet 192.28.137.9/24 brd 192.28.137.255 scope global eth0

controlplane ~ ➜  ipcalc -b 192.28.137.9
Address:   192.28.137.9         
Netmask:   255.255.255.0 = 24   
Wildcard:  0.0.0.255            
=>
Network:   192.28.137.0/24      
HostMin:   192.28.137.1         
HostMax:   192.28.137.254       
Broadcast: 192.28.137.255       
Hosts/Net: 254                   Class C
```

## for pod ranges
```bash
controlplane ~ ➜  k logs -n kube-system weave-net-8lthz  | grep ipalloc
Defaulted container "weave" out of: weave, weave-npc, weave-init (init)
INFO: 2023/07/19 14:45:23.691706 Command line options: map[conn-limit:200 datapath:datapath db-prefix:/weavedb/weave-net docker-api: expect-npc:true http-addr:127.0.0.1:6784 ipalloc-init:consensus=1 ipalloc-range:10.244.0.0/16 metrics-addr:0.0.0.0:6782 name:4a:ac:1d:10:22:2b nickname:node01 no-dns:true no-masq-local:true port:6783]

```

## for services IP ranges
```bash
cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range

#    - --service-cluster-ip-range=10.96.0.0/12
```

## for the kube-proxy method is it iptables or userspaces
```bash
controlplane ~ ➜  kubectl logs kube-proxy-84xnq -n kube-system
I0719 14:45:16.752250       1 node.go:141] Successfully retrieved node IP: 192.28.137.9
I0719 14:45:16.752439       1 server_others.go:110] "Detected node IP" address="192.28.137.9"
I0719 14:45:16.752512       1 server_others.go:551] "Using iptables proxy"
I0719 14:45:17.036258       1 server_others.go:190] "Using iptables Proxier"
I0719 14:45:17.036404       1 server_others.go:197] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0719 14:45:17.036423       1 server_others.go:198] "Creating dualStackProxier for iptables"
I0719 14:45:17.036468       1 server_others.go:481] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
```

> it is iptables

# coredns for dns records for kubernetes services

`cat /etc/coredns/Corefile`

so who adds the namesevr ip (i.e. service of coredns) its the kubelet

```yaml
# /var/lib/kubelet/config.yaml
...
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
```

but how does `demo` -> FQDN?

its
```conf
nameserver 10.96...
# this is added
search default.svc.cluster.local svc.cluster.local cluster.local
```

```bash
k get cm coredns -nkube-system -ojson | jq -r '.data.Corefile'
.:53 {
    errors
    health {
       lameduck 5s
    }
    ready
    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }
    prometheus :9153
    forward . /etc/resolv.conf {
       max_concurrent 1000
    }
    cache 30
    loop
    reload
    loadbalance
}
```

# ingress
path rewrites
https://kubernetes.github.io/ingress-nginx/examples/rewrite/

```
Create the ingress resource to make the applications available at /wear and /watch on the Ingress service.

Also, make use of rewrite-target annotation field: -

nginx.ingress.kubernetes.io/rewrite-target: /



Ingress resource comes under the namespace scoped, so don't forget to create the ingress in the app-space namespace.
```
